---
title: Facebook
spreadsheet: https://docs.google.com/spreadsheets/d/1jiRvb1AGuferhBb0s4lfOJbfDntKRJWRSO86YF9FyOI/edit#gid=2105002343
---
Facebook is a social network site with a multitude of function capabilities, though it primarily operates on users sharing posts with text, video, and links, as well as joining groups based on interests, events, or popular subjects. As of [Facebook’s Third Quarter in 2020](https://s21.q4cdn.com/399680738/files/doc_financials/2020/q3/FB-Q3-2020-Earnings-Presentation.pdf), Facebook has over 1.8 billion daily users. Content moderation on Facebook is a global and cross-network effort, as Facebook Company also owns other platforms including Instagram and WhatsApp. 

Facebook combats misinformation by moderating content in a three-part strategy: [Remove, Reduce, and Inform](https://about.fb.com/news/2019/04/remove-reduce-inform-new-steps/). [Third-Party Fact Checkers](https://www.facebook.com/business/help/182222309230722) are a large part of the Facebook content assessment, and are all a part of the [International Fact Checking Network](https://ifcncodeofprinciples.poynter.org/?fbclid=IwAR2Ua-AxU2j7isaiUqa3vS5ZPSrYSohaL2lPff5a71RSULIwAxZQ6hNxvfI) as a mandatory condition of working with Facebook. Under the Community Standards for Integrity & Authenticity, [False News](https://www.facebook.com/communitystandards/false_news)  is addressed, and their strategy is present in the following guidelines.  

1. Remove: In the general Community Standards, Facebook provides information on the content subject to removal - hate speech, violent and graphic content, adult nudity and sexual activity, sexual solicitation, and cruel and insensitive content. 
2. Reduce: Regarding false news, reduction includes (a) “Disrupting economic incentives for people, Pages and domains that propagate misinformation,” (b) “Using various signals, including feedback from our community, to inform a machine learning model that predicts which stories may be false,” (c) “Reducing the distribution of content rated as false by independent third-party fact-checkers.”
3. Inform: “Empowering people to decide for themselves what to read, trust and share by informing them with more context and promoting news literacy.” Strategies on informing, including the [Context Button](https://about.fb.com/news/2018/04/news-feed-fyi-more-context/), [Page Transparency](https://www.facebook.com/help/323314944866264/), and [Context Warning](https://about.fb.com/news/2019/10/update-on-election-integrity-efforts/) are included in these tables. 

**These strategies for content moderation and labeling are implemented by an internal Facebook review team, in tandem with reports and reviews from third-party fact check. There are three components of [content review on Facebook](https://about.fb.com/news/2020/08/how-we-review-content/). First, “proactive detection” which utilizes Artificial Intelligence to detect community violations; second, “automation” to assist with a large scale of platform content and enable human reviewers to address more particularly concerning or questionable content; and third, “prioritization” of reviewing content most likely to cause harm, become viral, or be most likely to violate community guidelines and platform policy.**